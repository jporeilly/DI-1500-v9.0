
# Advanced Pentaho Data Integration v8.3
  This course is designed to build upon your fundamental knowledge of Pentaho Data Integration (PDI).   
  Moving beyond the basics of creating transformations and jobs, you will learn how to use PDI in real-world project scenarios.
  You'll add PDI as a data source for a variety of visualization options, utilize PDI's streaming data processing capabilties, build transformations with metadata injection, and scale and performance tune your PDI solution.

### Prerequisites

The following software need to be installed and configured:
```
Pentaho Business Analytics 9.0.x
OpenJDK 14
Apache Kafka 2.5.0
Kafka Tool 2.0.4
MQTT.fx 1.71
Visual Studio Code
R 4.0.0
RStudio
Python 3.8.3
```

### Course Overview

On completing this course, you will be able to:

#### Module 1 - Metadata Injection
```
  Overview of Metadata Injection
  * Metadata Injection Workflows
    - Standard
    - Push / Pull
    - 2-phase
    - Filters
  * Use Case - Retail Sales
```

#### Module 2 - PDI as a Data Source
```
  Configure PDI as a datasource for various scenarios:
  * CDA
  * Google BQ & Drive
  * Snowflake
  * Data Services
  * Machine Learning
```  

#### Module 3 - Streaming Data
```
  Stream SensorData to:
  * MQTT Broker
  * Kafka
  * Amazon Kinesis
```

#### Module 4 - Scalability
```
  Configuring Master & Slave nodes
  * Clustering
  * Partitioning
  Scheduling
  Checkpoints
```

### Getting Started

[Course Materials](https://jporeilly.github.io/Pentaho-Training/scripts/DI-1500_v9.0.cmd) - Batch script for GitHub repositories *Requires Git to be installed.

